{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver import Chrome\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser = Chrome() #open chrome in selenium\n",
    "browser.get('https://meduza.io/articles') # open page with a lot of urls to ads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_ = []\n",
    "links = browser.find_elements_by_xpath('//*[@href]') # find all urls on page\n",
    "for ii in links:\n",
    "    list_.append(ii.get_attribute('href')) #append all hrefs to a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "meduza_ads = []\n",
    "for item in list_:\n",
    "    if item[:25] == 'https://meduza.io/feature': # get rid of non-ad urls\n",
    "        meduza_ads.append(item)\n",
    "meduza_ads_ = meduza_ads[1:-5] # get rid of not important urls like: how meduza's using coockie, etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "892"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(meduza_ads_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = [] #create list\n",
    "df = pd.DataFrame() #create df\n",
    "for item in meduza_ads_:\n",
    "    r = requests.get(str(item)) #open url \n",
    "    soup = BeautifulSoup(r.text, 'html5lib') # get html as text\n",
    "    doc = {} #create dictionary\n",
    "    if not soup.find('div', {'class' : 'Lead'}):\n",
    "        continue\n",
    "    else:\n",
    "        doc['text'] = soup.find('div', {'class' : 'Lead'}).text #get text from html text\n",
    "        doc['txt'] = soup.find('div', {'class' : 'Body'}).text #get text from html text\n",
    "        text = ''.join((doc['text'], doc['txt'])) # join 2 string to 1\n",
    "    p.append(text) #every iterate we append all texts to list\n",
    "    df = pd.DataFrame.from_records(({\"value\": i} for i in p)) #convert list to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('parsed_nonad_data.csv', index=False) #save df to csv\n",
    "a = pd.read_csv('parsed_nonad_data.csv') # check csv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
